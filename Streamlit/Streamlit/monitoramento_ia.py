import streamlit as st
import pandas as pd
import random
import datetime
from openai import OpenAI

# CONFIGURAÃ‡ÃƒO DA PÃGINA

st.set_page_config(
    page_title="Monitoramento de IA", 
    layout="wide",
    page_icon="ðŸ¤–"
)

# SIDEBAR - CONTROLES E INFORMAÃ‡Ã•ES
with st.sidebar:
    st.title("ConfiguraÃ§Ãµes")
    openai_api_key = st.text_input(
        "ðŸ”‘ Chave da API OpenAI:", 
        type="password",
        help="Obtenha em platform.openai.com"
    )
    
    # Status da API
    api_status = st.empty()
    
    st.markdown("---")
    st.caption("â„¹ï¸ Tokens sÃ£o calculados automaticamente")
    st.caption("ðŸ’¡ Modelo padrÃ£o: gpt-3.5-turbo")

# DASHBOARD DE MONITORAMENTO

st.title("ðŸ“Š Monitoramento de Modelos de IA")

@st.cache_data
def gerar_dados_modelo():
    """Gera dados simulados de performance de modelo"""
    data = []
    for i in range(10):
        data.append({
            "Data": (datetime.datetime.now() - datetime.timedelta(days=i)).strftime("%d/%m/%Y"),
            "Modelo": f"Modelo {random.choice(['A', 'B'])}",
            "AcurÃ¡cia": round(random.uniform(0.85, 0.99), 2),
            "LatÃªncia (ms)": round(random.uniform(80, 200), 1),
            "Chamadas": random.randint(100, 1000),
        })
    return pd.DataFrame(data[::-1])

# Gerar e mostrar dados
df = gerar_dados_modelo()

# MÃ©tricas rÃ¡pidas
col1, col2, col3 = st.columns(3)
col1.metric("AcurÃ¡cia MÃ©dia", f"{df['AcurÃ¡cia'].mean():.2%}")
col2.metric("LatÃªncia MÃ©dia", f"{df['LatÃªncia (ms)'].mean():.1f} ms")
col3.metric("Total Chamadas", df['Chamadas'].sum())

# Tabela e grÃ¡ficos
tab1, tab2 = st.tabs(["ðŸ“Š Dados Detalhados", "ðŸ“ˆ VisualizaÃ§Ãµes"])
with tab1:
    st.dataframe(df, use_container_width=True)
with tab2:
    st.line_chart(df, x="Data", y=["AcurÃ¡cia", "LatÃªncia (ms)"], color=["#4CAF50", "#FF5722"])


st.markdown("---")
st.header("ðŸ¤– Assistente de IA")

# InicializaÃ§Ã£o do historico
if "historico" not in st.session_state:
    st.session_state.historico = []

# Mostrar historico existente
for msg in st.session_state.historico:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# InteraÃ§Ã£o do usuÃ¡rio
if prompt := st.chat_input("Como posso ajudar com seu modelo de IA?"):
    # Atualiza status da API
    with st.sidebar:
        if openai_api_key:
            api_status.success("âœ… API Conectada", icon="ðŸ”Œ")
    
    # Adiciona pergunta ao histÃ³rico
    st.session_state.historico.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    
    with st.chat_message("assistant"):
        with st.spinner("Analisando sua consulta..."):
            try:
                # Configura cliente openAI
                client = OpenAI(api_key=openai_api_key)
                
                # API
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo", # pode mudar pro modelo que quiser.
                    messages=[
                        {
                            "role": "system", 
                            "content": """
                            VocÃª Ã© um especialista em machine learning. Responda de forma:
                            - TÃ©cnica mas acessÃ­vel
                            - Com exemplos prÃ¡ticos quando relevante
                            - MÃ¡ximo 300 tokens
                            """
                        },
                        *[{"role": m["role"], "content": m["content"]} for m in st.session_state.historico]
                    ],
                    temperature=0.7,
                    max_tokens=300
                )
                
                resposta = response.choices[0].message.content
                st.markdown(resposta)
                st.session_state.historico.append({"role": "assistant", "content": resposta})
                
                # Mostra estatÃ­sticas de uso
                tokens = response.usage.total_tokens
                with st.sidebar:
                    st.info(f"ðŸ”¢ Tokens usados: {tokens} (â‰ˆ ${tokens*0.000002:.4f})")
            
            except Exception as e:
                erro = f"""
                âš ï¸ **Erro na API**  
                {str(e)}  
                âˆ™ Verifique sua [chave e quota](https://platform.openai.com/usage)  
                âˆ™ Modelo alternativo: `gpt-3.5-turbo`
                """
                st.error(erro)
                st.session_state.historico.append({"role": "assistant", "content": erro})
    
    # Limita histÃ³rico (3 Ãºltimas conversas)
    # VVocÃª pode limitar o nÃºmero de mensagens enviadas Ã  API (isso afeta custo e performance).
    if len(st.session_state.historico) > 6:
        st.session_state.historico = st.session_state.historico[-6:]


# RODAPÃ‰
st.markdown("---")
st.caption("ðŸ“Œ Monitoramento em tempo real - Atualizado em " + 
          datetime.datetime.now().strftime("%d/%m/%Y %H:%M"))